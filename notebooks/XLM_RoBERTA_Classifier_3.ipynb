{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-OSR609XrmlM",
        "outputId": "771b5184-3578-418c-bb1b-e1b00b4d7b9c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LhlHIeO5K443"
      },
      "outputs": [],
      "source": [
        "# A dependency of the preprocessing for BERT inputs\n",
        "!pip install -q -U \"tensorflow-text==2.8.*\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y07Vop9VUJlK"
      },
      "outputs": [],
      "source": [
        "!pip install -q tf-models-official==2.7.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pxmrsOJCZ0Q1",
        "outputId": "5a30e728-513f-4aa2-943c-f73f0618a582"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/ensure_tf_install.py:53: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.9.0 and strictly below 2.12.0 (nightly versions are not supported). \n",
            " The versions of TensorFlow you are currently using is 2.8.4 and is not supported. \n",
            "Some things might work, some things might not.\n",
            "If you were to encounter a bug, do not file an issue.\n",
            "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
            "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
            "https://github.com/tensorflow/addons\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_text as text\n",
        "from official.nlp import optimization  # to create AdamW optimizer\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "tf.get_logger().setLevel('ERROR')\n",
        "\n",
        "from keras import backend as K \n",
        "from natsort import natsorted"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bo19QGpw72Ek"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "train_all = pd.read_csv('drive/MyDrive/ML/texts_with_skills_one-hot.csv', sep = \"|\", encoding = \"UTF-8\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6vH0t1_mFqA_"
      },
      "outputs": [],
      "source": [
        "#train_all.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WqQ_jFw_Fh54"
      },
      "outputs": [],
      "source": [
        "train_all_X = train_all['text']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3KkUIhg1F13M"
      },
      "outputs": [],
      "source": [
        "#bert_preprocess_model = hub.KerasLayer('https://tfhub.dev/jeongukjae/xlm_roberta_multi_cased_preprocess/1')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9B911nvEz9H7"
      },
      "outputs": [],
      "source": [
        "#text_preprocessed = bert_preprocess_model(train_all_X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x8QQDSGMzhXr"
      },
      "outputs": [],
      "source": [
        "#bert_model = hub.KerasLayer('https://tfhub.dev/jeongukjae/xlm_roberta_multi_cased_L-12_H-768_A-12/1')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ivJZSwmnDsqe"
      },
      "outputs": [],
      "source": [
        "def build_classifier_model():\n",
        "  text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')\n",
        "  preprocessing_layer = hub.KerasLayer('https://tfhub.dev/jeongukjae/xlm_roberta_multi_cased_preprocess/1', name='preprocessing')\n",
        "  encoder_inputs = preprocessing_layer(text_input)\n",
        "  encoder = hub.KerasLayer('https://tfhub.dev/jeongukjae/xlm_roberta_multi_cased_L-12_H-768_A-12/1', trainable=True, name='BERT_encoder')\n",
        "  outputs = encoder(encoder_inputs)\n",
        "  net = outputs['pooled_output']\n",
        "  net = tf.keras.layers.Dropout(0.1)(net)\n",
        "  net = tf.keras.layers.Dense(1, activation=None, name='classifier')(net)\n",
        "  return tf.keras.Model(text_input, net)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JKuHCUrYFa_u"
      },
      "outputs": [],
      "source": [
        "#classifier_model = build_classifier_model()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6-14N6X_I-Y3"
      },
      "outputs": [],
      "source": [
        "#loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "#metrics = tf.metrics.BinaryAccuracy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2MmToAmtI_C5"
      },
      "outputs": [],
      "source": [
        "#epochs = 10\n",
        "#steps_per_epoch = 64\n",
        "#num_train_steps = steps_per_epoch * epochs\n",
        "#num_warmup_steps = int(0.1*num_train_steps)\n",
        "\n",
        "#init_lr = 3e-5\n",
        "#optimizer = optimization.create_optimizer(init_lr=init_lr,\n",
        "#                                          num_train_steps=num_train_steps,\n",
        "#                                          num_warmup_steps=num_warmup_steps,\n",
        "#                                          optimizer_type='adamw')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xA37eGaTJBMx"
      },
      "outputs": [],
      "source": [
        "#classifier_model.compile(optimizer=optimizer,\n",
        "#                         loss=loss,\n",
        "#                         metrics=metrics)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TxQhGrzNeKX9"
      },
      "outputs": [],
      "source": [
        "#history = classifier_model.fit(x = train_all_X, y = train_0_y, epochs = epochs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DVMvzsUJmShn"
      },
      "source": [
        "Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mWEBv8W6iw8G"
      },
      "outputs": [],
      "source": [
        "test_all = pd.read_csv('drive/MyDrive/ML/test_texts_with_ids.csv', sep = \"|\", encoding = \"UTF-8\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EeczsGfSmWVx"
      },
      "outputs": [],
      "source": [
        "test_texts = test_all[\"text\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MzGvDYXmnFqb"
      },
      "outputs": [],
      "source": [
        "#for i in tf.sigmoid(classifier_model(tf.constant(test_texts))):\n",
        "#    print(i)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a9Ugvauy4357"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-w8HEs0543F8"
      },
      "outputs": [],
      "source": [
        "to_drop = []\n",
        "\n",
        "directory = 'drive/MyDrive/ML/preds/15'\n",
        "\n",
        "for filename in natsorted(os.listdir(directory)):\n",
        "    f = os.path.join(directory, filename)\n",
        "    # checking if it is a file\n",
        "    if os.path.isfile(f):\n",
        "        skill = f.strip().split(\".\")[0].split(\"_\")[-1]\n",
        "        to_drop.append(skill)\n",
        "\n",
        "skill_list = train_all.keys().drop('text').drop(to_drop).tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lfDmiSVq53kY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df4b2c93-1de0-4cd1-a375-0fdae45e89e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class = s25\n",
            "Epoch 1/15\n",
            "7/7 [==============================] - 343s 45s/step - loss: 0.7718 - binary_accuracy: 0.9704\n",
            "Epoch 2/15\n",
            "7/7 [==============================] - 335s 48s/step - loss: 0.7187 - binary_accuracy: 0.9704\n",
            "Epoch 3/15\n",
            "7/7 [==============================] - 315s 44s/step - loss: 0.7120 - binary_accuracy: 0.9704\n",
            "Epoch 4/15\n",
            "7/7 [==============================] - 330s 47s/step - loss: 0.7342 - binary_accuracy: 0.9704\n",
            "Epoch 5/15\n",
            "7/7 [==============================] - 321s 45s/step - loss: 0.7463 - binary_accuracy: 0.9704\n",
            "Epoch 6/15\n",
            "7/7 [==============================] - 333s 47s/step - loss: 0.7640 - binary_accuracy: 0.9704\n",
            "Done\n",
            "Class = s26\n",
            "Epoch 1/15\n"
          ]
        }
      ],
      "source": [
        "skills_in_test = []\n",
        "\n",
        "\n",
        "for skill in skill_list:\n",
        "    K.clear_session()\n",
        "    tf.keras.backend.clear_session()\n",
        "    \n",
        "    print(\"Class =\", skill)\n",
        "    classifier_model = build_classifier_model()\n",
        "    loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "    metrics = tf.metrics.BinaryAccuracy()\n",
        "\n",
        "    callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3)\n",
        "\n",
        "    epochs = 15\n",
        "    steps_per_epoch = 64\n",
        "    num_train_steps = steps_per_epoch * epochs\n",
        "    num_warmup_steps = int(0.1*num_train_steps)\n",
        "\n",
        "    init_lr = 3e-5\n",
        "    optimizer = optimization.create_optimizer(init_lr=init_lr,\n",
        "                                            num_train_steps=num_train_steps,\n",
        "                                            num_warmup_steps=num_warmup_steps,\n",
        "                                            optimizer_type='adamw')\n",
        "    \n",
        "    classifier_model.compile(optimizer=optimizer,\n",
        "                            loss=loss,\n",
        "                            metrics=metrics)\n",
        "    \n",
        "    train_y = train_all[skill]\n",
        "\n",
        "    total = len(train_y)\n",
        "    pos = train_y.value_counts()[1]\n",
        "    neg = train_y.value_counts()[0]\n",
        "\n",
        "    weight_for_0 = (1 / neg) * (total / 2.0)\n",
        "    weight_for_1 = (1 / pos) * (total / 2.0)\n",
        "\n",
        "    class_weight = {0: weight_for_0, 1: weight_for_1}\n",
        "\n",
        "    history = classifier_model.fit(x = train_all_X, y = train_y, epochs = epochs, class_weight = class_weight, callbacks=[callback])\n",
        "\n",
        "    filename = \"drive/MyDrive/ML/preds/15/test_preds_\" + skill + \".txt\"\n",
        "    with open(filename, 'w') as fp:\n",
        "        for item in tf.sigmoid(classifier_model(tf.constant(test_texts))).numpy().tolist():\n",
        "            fp.write(\"%s\\n\" % item[0])\n",
        "        print('Done')\n",
        "\n",
        "    del classifier_model\n",
        "    K.clear_session()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hDNNV5zfEgbb"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AvkOaWFLEhB8"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from natsort import natsorted\n",
        "# assign directory\n",
        "directory = 'drive/MyDrive/ML/preds/15'\n",
        "\n",
        "skills_preds = pd.DataFrame()\n",
        "\n",
        "# iterate over files in\n",
        "# that directory\n",
        "for filename in natsorted(os.listdir(directory)):\n",
        "    f = os.path.join(directory, filename)\n",
        "    # checking if it is a file\n",
        "    if os.path.isfile(f):\n",
        "        skill = f.strip().split(\".\")[0].split(\"_\")[-1]\n",
        "        with open(f) as file:\n",
        "          df = file.readlines()\n",
        "          skills_preds[skill] = [float(i) for i in df]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S_3IQXKoLfAJ"
      },
      "outputs": [],
      "source": [
        "truths = pd.concat([test_all.drop('text', axis = 1), skills_preds >= 0.5], axis = 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A9TplvAQNzJX"
      },
      "outputs": [],
      "source": [
        "thingie = []\n",
        "\n",
        "for index, row in truths.iterrows():\n",
        "  id = row['text_id']\n",
        "\n",
        "  skl = row.drop('text_id')\n",
        "\n",
        "  true_skills = skl[skl == True]\n",
        "  \n",
        "  if len(true_skills) == 0:\n",
        "    highest_idx = skills_preds.iloc[index].idxmax()\n",
        "    true_skills = [highest_idx]\n",
        "  else:\n",
        "    true_skills = true_skills.keys().to_list()\n",
        "\n",
        "  skill_list = \" \".join(true_skills)\n",
        "  thingie.append(\",\".join([id, skill_list]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pqBvEoDcPiKC"
      },
      "outputs": [],
      "source": [
        "with open(\"drive/MyDrive/ML/XLM_RoBERTa_15e_withBest.csv\", \"w\") as file:\n",
        "  file.write(\"text_id,skills\\n\")\n",
        "  for line in thingie:\n",
        "    file.write(line)\n",
        "    file.write(\"\\n\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}