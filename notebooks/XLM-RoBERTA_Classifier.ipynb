{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# A dependency of the preprocessing for BERT inputs\n",
        "!pip install -q -U \"tensorflow-text==2.8.*\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LhlHIeO5K443",
        "outputId": "6487dc01-429d-47ad-f455-2e7e7d6e4afb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 4.9 MB 28.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 498.0 MB 13 kB/s \n",
            "\u001b[K     |████████████████████████████████| 5.8 MB 69.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 462 kB 74.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.4 MB 58.2 MB/s \n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q tf-models-official==2.7.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y07Vop9VUJlK",
        "outputId": "960d98b6-d2a0-404c-bb41-def758538d3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 1.8 MB 41.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 352 kB 75.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 61.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 71.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 238 kB 84.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 43 kB 2.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 118 kB 65.1 MB/s \n",
            "\u001b[?25h  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_text as text\n",
        "from official.nlp import optimization  # to create AdamW optimizer\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "tf.get_logger().setLevel('ERROR')\n",
        "\n",
        "from keras import backend as K "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pxmrsOJCZ0Q1",
        "outputId": "f1bc5f01-de39-43d5-f985-28dfe6388b49"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/ensure_tf_install.py:53: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.9.0 and strictly below 2.12.0 (nightly versions are not supported). \n",
            " The versions of TensorFlow you are currently using is 2.8.4 and is not supported. \n",
            "Some things might work, some things might not.\n",
            "If you were to encounter a bug, do not file an issue.\n",
            "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
            "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
            "https://github.com/tensorflow/addons\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "train_all = pd.read_csv('texts_with_skills_one-hot.csv', sep = \"|\", encoding = \"UTF-8\")"
      ],
      "metadata": {
        "id": "bo19QGpw72Ek"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#train_all.head()"
      ],
      "metadata": {
        "id": "6vH0t1_mFqA_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_all_X = train_all['text']"
      ],
      "metadata": {
        "id": "WqQ_jFw_Fh54"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#bert_preprocess_model = hub.KerasLayer('https://tfhub.dev/jeongukjae/xlm_roberta_multi_cased_preprocess/1')"
      ],
      "metadata": {
        "id": "3KkUIhg1F13M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#text_preprocessed = bert_preprocess_model(train_all_X)"
      ],
      "metadata": {
        "id": "9B911nvEz9H7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#bert_model = hub.KerasLayer('https://tfhub.dev/jeongukjae/xlm_roberta_multi_cased_L-12_H-768_A-12/1')"
      ],
      "metadata": {
        "id": "x8QQDSGMzhXr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_classifier_model():\n",
        "  text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')\n",
        "  preprocessing_layer = hub.KerasLayer('https://tfhub.dev/jeongukjae/xlm_roberta_multi_cased_preprocess/1', name='preprocessing')\n",
        "  encoder_inputs = preprocessing_layer(text_input)\n",
        "  encoder = hub.KerasLayer('https://tfhub.dev/jeongukjae/xlm_roberta_multi_cased_L-12_H-768_A-12/1', trainable=True, name='BERT_encoder')\n",
        "  outputs = encoder(encoder_inputs)\n",
        "  net = outputs['pooled_output']\n",
        "  net = tf.keras.layers.Dropout(0.1)(net)\n",
        "  net = tf.keras.layers.Dense(1, activation=None, name='classifier')(net)\n",
        "  return tf.keras.Model(text_input, net)"
      ],
      "metadata": {
        "id": "ivJZSwmnDsqe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#classifier_model = build_classifier_model()"
      ],
      "metadata": {
        "id": "JKuHCUrYFa_u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "#metrics = tf.metrics.BinaryAccuracy()"
      ],
      "metadata": {
        "id": "6-14N6X_I-Y3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#epochs = 10\n",
        "#steps_per_epoch = 64\n",
        "#num_train_steps = steps_per_epoch * epochs\n",
        "#num_warmup_steps = int(0.1*num_train_steps)\n",
        "\n",
        "#init_lr = 3e-5\n",
        "#optimizer = optimization.create_optimizer(init_lr=init_lr,\n",
        "#                                          num_train_steps=num_train_steps,\n",
        "#                                          num_warmup_steps=num_warmup_steps,\n",
        "#                                          optimizer_type='adamw')"
      ],
      "metadata": {
        "id": "2MmToAmtI_C5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#classifier_model.compile(optimizer=optimizer,\n",
        "#                         loss=loss,\n",
        "#                         metrics=metrics)"
      ],
      "metadata": {
        "id": "xA37eGaTJBMx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#history = classifier_model.fit(x = train_all_X, y = train_0_y, epochs = epochs)"
      ],
      "metadata": {
        "id": "TxQhGrzNeKX9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test"
      ],
      "metadata": {
        "id": "DVMvzsUJmShn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_all = pd.read_csv('test_texts_with_ids.csv', sep = \"|\", encoding = \"UTF-8\")"
      ],
      "metadata": {
        "id": "mWEBv8W6iw8G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_texts = test_all[\"text\"]"
      ],
      "metadata": {
        "id": "EeczsGfSmWVx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#for i in tf.sigmoid(classifier_model(tf.constant(test_texts))):\n",
        "#    print(i)"
      ],
      "metadata": {
        "id": "MzGvDYXmnFqb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "a9Ugvauy4357"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#to_drop = ['s0', 's1', 's10', 's11', 's12', 's13', 's15', 's16', 's17', 's18', 's19', 's2']\n",
        "skill_list = train_all.keys().drop('text').drop(to_drop).tolist()"
      ],
      "metadata": {
        "id": "-w8HEs0543F8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "skills_in_test = []\n",
        "\n",
        "for skill in skill_list:\n",
        "    K.clear_session()\n",
        "    tf.keras.backend.clear_session()\n",
        "    tf.reset_default_graph()\n",
        "    \n",
        "    print(\"Class =\", skill)\n",
        "    classifier_model = build_classifier_model()\n",
        "    loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "    metrics = tf.metrics.BinaryAccuracy()\n",
        "\n",
        "    epochs = 10\n",
        "    steps_per_epoch = 64\n",
        "    num_train_steps = steps_per_epoch * epochs\n",
        "    num_warmup_steps = int(0.1*num_train_steps)\n",
        "\n",
        "    init_lr = 3e-5\n",
        "    optimizer = optimization.create_optimizer(init_lr=init_lr,\n",
        "                                            num_train_steps=num_train_steps,\n",
        "                                            num_warmup_steps=num_warmup_steps,\n",
        "                                            optimizer_type='adamw')\n",
        "    \n",
        "    classifier_model.compile(optimizer=optimizer,\n",
        "                            loss=loss,\n",
        "                            metrics=metrics)\n",
        "    \n",
        "    train_y = train_all[skill]\n",
        "\n",
        "    total = len(train_y)\n",
        "    pos = train_y.value_counts()[1]\n",
        "    neg = train_y.value_counts()[0]\n",
        "\n",
        "    weight_for_0 = (1 / neg) * (total / 2.0)\n",
        "    weight_for_1 = (1 / pos) * (total / 2.0)\n",
        "\n",
        "    class_weight = {0: weight_for_0, 1: weight_for_1}\n",
        "\n",
        "    history = classifier_model.fit(x = train_all_X, y = train_y, epochs = epochs, class_weight = class_weight)\n",
        "\n",
        "    filename = \"test_preds_\" + skill + \".txt\"\n",
        "    with open(filename, 'w') as fp:\n",
        "        for item in tf.sigmoid(classifier_model(tf.constant(test_texts))).numpy().tolist():\n",
        "            fp.write(\"%s\\n\" % item[0])\n",
        "        print('Done')\n",
        "\n",
        "    del classifier_model\n",
        "    K.clear_session()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "lfDmiSVq53kY",
        "outputId": "04f2b008-5d26-46d2-908d-84b034ec1dad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class = s18\n",
            "Epoch 1/10\n",
            "7/7 [==============================] - 23s 899ms/step - loss: 0.7031 - binary_accuracy: 0.8079\n",
            "Epoch 2/10\n",
            "7/7 [==============================] - 6s 910ms/step - loss: 0.7055 - binary_accuracy: 0.9015\n",
            "Epoch 3/10\n",
            "7/7 [==============================] - 6s 881ms/step - loss: 0.7075 - binary_accuracy: 0.9212\n",
            "Epoch 4/10\n",
            "7/7 [==============================] - 6s 883ms/step - loss: 0.6887 - binary_accuracy: 0.9212\n",
            "Epoch 5/10\n",
            "7/7 [==============================] - 6s 893ms/step - loss: 0.6647 - binary_accuracy: 0.9507\n",
            "Epoch 6/10\n",
            "7/7 [==============================] - 7s 1s/step - loss: 0.6263 - binary_accuracy: 0.9507\n",
            "Epoch 7/10\n",
            "7/7 [==============================] - 6s 900ms/step - loss: 0.7764 - binary_accuracy: 0.9507\n",
            "Epoch 8/10\n",
            "7/7 [==============================] - 7s 976ms/step - loss: 0.7711 - binary_accuracy: 0.9507\n",
            "Epoch 9/10\n",
            "7/7 [==============================] - 7s 918ms/step - loss: 0.6469 - binary_accuracy: 0.9507\n",
            "Epoch 10/10\n",
            "7/7 [==============================] - 7s 922ms/step - loss: 0.5889 - binary_accuracy: 0.9507\n",
            "Done\n",
            "Class = s19\n",
            "Epoch 1/10\n",
            "7/7 [==============================] - 20s 895ms/step - loss: 0.7187 - binary_accuracy: 0.9163\n",
            "Epoch 2/10\n",
            "7/7 [==============================] - 6s 905ms/step - loss: 0.6909 - binary_accuracy: 0.9557\n",
            "Epoch 3/10\n",
            "7/7 [==============================] - 6s 910ms/step - loss: 0.7107 - binary_accuracy: 0.9557\n",
            "Epoch 4/10\n",
            "7/7 [==============================] - 6s 914ms/step - loss: 0.7221 - binary_accuracy: 0.9606\n",
            "Epoch 5/10\n",
            "7/7 [==============================] - 7s 920ms/step - loss: 0.6784 - binary_accuracy: 0.9606\n",
            "Epoch 6/10\n",
            "7/7 [==============================] - 7s 918ms/step - loss: 0.7030 - binary_accuracy: 0.9606\n",
            "Epoch 7/10\n",
            "7/7 [==============================] - 6s 909ms/step - loss: 0.6649 - binary_accuracy: 0.9606\n",
            "Epoch 8/10\n",
            "7/7 [==============================] - 6s 902ms/step - loss: 1.0144 - binary_accuracy: 0.9606\n",
            "Epoch 9/10\n",
            "7/7 [==============================] - 6s 897ms/step - loss: 0.6633 - binary_accuracy: 0.9606\n",
            "Epoch 10/10\n",
            "7/7 [==============================] - 6s 896ms/step - loss: 0.3898 - binary_accuracy: 0.9507\n",
            "Done\n",
            "Class = s2\n",
            "Epoch 1/10\n",
            "7/7 [==============================] - 21s 900ms/step - loss: 0.7051 - binary_accuracy: 0.9015\n",
            "Epoch 2/10\n",
            "7/7 [==============================] - 6s 899ms/step - loss: 0.7038 - binary_accuracy: 0.9015\n",
            "Epoch 3/10\n",
            "7/7 [==============================] - 6s 906ms/step - loss: 0.7013 - binary_accuracy: 0.8966\n",
            "Epoch 4/10\n",
            "7/7 [==============================] - 6s 914ms/step - loss: 0.7040 - binary_accuracy: 0.8966\n",
            "Epoch 5/10\n",
            "7/7 [==============================] - 7s 923ms/step - loss: 0.6326 - binary_accuracy: 0.9113\n",
            "Epoch 6/10\n",
            "7/7 [==============================] - 7s 916ms/step - loss: 0.6707 - binary_accuracy: 0.8818\n",
            "Epoch 7/10\n",
            "7/7 [==============================] - 7s 928ms/step - loss: 0.5466 - binary_accuracy: 0.9163\n",
            "Epoch 8/10\n",
            "7/7 [==============================] - 7s 965ms/step - loss: 0.4232 - binary_accuracy: 0.8916\n",
            "Epoch 9/10\n",
            "7/7 [==============================] - 6s 906ms/step - loss: 0.3928 - binary_accuracy: 0.9310\n",
            "Epoch 10/10\n",
            "7/7 [==============================] - 6s 907ms/step - loss: 0.3133 - binary_accuracy: 0.9360\n",
            "Done\n",
            "Class = s20\n",
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ResourceExhaustedError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-01af744b536d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0mclass_weight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mweight_for_0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mweight_for_1\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m     \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassifier_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_all_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_weight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclass_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"test_preds_\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mskill\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\".txt\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     52\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     55\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     56\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mResourceExhaustedError\u001b[0m: Graph execution error:\n\nfailed to allocate memory\n\t [[{{node transformer/layer_8/self_attention_layer_norm/batchnorm/mul_2}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n [Op:__inference_train_function_282153]"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "hDNNV5zfEgbb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from natsort import natsorted\n",
        "# assign directory\n",
        "directory = 'preds'\n",
        "\n",
        "skills_preds = pd.DataFrame()\n",
        "\n",
        "# iterate over files in\n",
        "# that directory\n",
        "for filename in natsorted(os.listdir(directory)):\n",
        "    f = os.path.join(directory, filename)\n",
        "    # checking if it is a file\n",
        "    if os.path.isfile(f):\n",
        "        skill = f.strip().split(\".\")[0].split(\"_\")[-1]\n",
        "        with open(f) as file:\n",
        "          df = file.readlines()\n",
        "          skills_preds[skill] = [float(i) for i in df]"
      ],
      "metadata": {
        "id": "AvkOaWFLEhB8"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "truths = pd.concat([test_all.drop('text', axis = 1), skills_preds >= 0.5], axis = 1)"
      ],
      "metadata": {
        "id": "S_3IQXKoLfAJ"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "thingie = []\n",
        "\n",
        "for index, row in truths.iterrows():\n",
        "  id = row['text_id']\n",
        "\n",
        "  skl = row.drop('text_id')\n",
        "\n",
        "  true_skills = skl[skl == True]\n",
        "  \n",
        "  if len(true_skills) == 0:\n",
        "    highest_idx = skills_preds.iloc[index].idxmax()\n",
        "    true_skills = [highest_idx]\n",
        "  else:\n",
        "    true_skills = true_skills.keys().to_list()\n",
        "\n",
        "  skill_list = \" \".join(true_skills)\n",
        "  thingie.append(\",\".join([id, skill_list]))"
      ],
      "metadata": {
        "id": "A9TplvAQNzJX"
      },
      "execution_count": 126,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"XLM_RoBERTa_10e_withBest.csv\", \"w\") as file:\n",
        "  file.write(\"text_id,skills\\n\")\n",
        "  for line in thingie:\n",
        "    file.write(line)\n",
        "    file.write(\"\\n\")"
      ],
      "metadata": {
        "id": "pqBvEoDcPiKC"
      },
      "execution_count": 127,
      "outputs": []
    }
  ]
}